{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa45bebf",
   "metadata": {},
   "source": [
    "# Python coding interview question with Solutions\n",
    "\n",
    "\n",
    "\n",
    "Challenge: Implement a method that extract distinct words from a string.\n",
    "\n",
    "For example, the function input is a string \"every person had a star, and every star had a also a friend\" and returns \"every person had a star friend\"\n",
    "\n",
    "\n",
    "Solution 1 : We want a hash map to keep track of words in a string. The appropriate data structure is a dictionary, where the words are keys. \n",
    "\n",
    "Manipulating  strings (texts) at the level of individual words implies that we first divided the string into individual\n",
    "words. So, the first task is to extract the words from a string:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a45b0b",
   "metadata": {},
   "source": [
    "## Solution 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe269eff",
   "metadata": {},
   "source": [
    "We can split a string on whitespace using the Python built-in string split() method: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "435eb7bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['every',\n",
       " 'person',\n",
       " 'had',\n",
       " 'a',\n",
       " 'star,',\n",
       " 'and',\n",
       " 'every',\n",
       " 'star',\n",
       " 'had',\n",
       " 'a',\n",
       " 'also',\n",
       " 'a',\n",
       " 'friend']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = \"every person had a star, and every star had a also a friend\" \n",
    "l = s.split( )\n",
    "l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6b05e6",
   "metadata": {},
   "source": [
    "Strings can contain sentences and any number of spaces, tabs, or newlines -  better way is to use regex split() method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67ea2168",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['every person had a star, and every star had a also a friend']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Match any number of tabs, newlines or spaces:\n",
    "\n",
    "l = re.split(r'[\\t\\n]+ ', s)\n",
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b488400e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['every',\n",
       " 'person',\n",
       " 'had',\n",
       " 'a',\n",
       " 'star,',\n",
       " 'and',\n",
       " 'every',\n",
       " 'star',\n",
       " 'had',\n",
       " 'a',\n",
       " 'also',\n",
       " 'a',\n",
       " 'friend']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Alternatively, march any number of whitespace characters:\n",
    "l =re.split(r'\\s+', s)\n",
    "l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "071f77de",
   "metadata": {},
   "source": [
    "Since strings can contain sentences, and any number of whitespaces as well a punctuation marks, an even better choice \n",
    "is using regex split() method matching all non-word characters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de2743f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['every',\n",
       " 'person',\n",
       " 'had',\n",
       " 'a',\n",
       " 'star',\n",
       " 'and',\n",
       " 'every',\n",
       " 'star',\n",
       " 'had',\n",
       " 'a',\n",
       " 'also',\n",
       " 'a',\n",
       " 'friend']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = re.split(r'\\W+',s)\n",
    "l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e6def62",
   "metadata": {},
   "source": [
    "### REMARK\n",
    "\n",
    "The task of extracting words from a text (string) aka tokenization turns out to be a far more complex \n",
    "than this example shows. In fact, there is no single solution to tokenization that works well in all real-world situations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a7c5ef",
   "metadata": {},
   "source": [
    "The function takes a string s and returns the keys of dictionary that contains the distinct words extracted from the string s:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05a06c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Solution 1:\n",
    "\n",
    "d ={}\n",
    "\n",
    "def f1(s):\n",
    "    l = re.split(r'\\W+',s)\n",
    "    if l == []:\n",
    "        return 0\n",
    "    for w in l:\n",
    "        d[w] = w\n",
    "    return list(d.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20c7bec6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['every', 'person', 'had', 'a', 'star', 'and', 'also', 'friend']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b96f5d",
   "metadata": {},
   "source": [
    "## Solution 2\n",
    "\n",
    "The question here is in fact about finding the vocabulary of a text (string) - the set of words that used in the text.\n",
    "\n",
    "The easiest way to get the vocabulary items is using Python set() method that dedupes the list of words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4521425",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a', 'also', 'and', 'every', 'friend', 'had', 'person', 'star'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9717846a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Solution 2\n",
    "\n",
    "def f2(s):\n",
    "    l = re.split(r'\\W+',s)\n",
    "    return (list(set(l)))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "02f1889f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['person', 'also', 'and', 'a', 'star', 'every', 'had', 'friend']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f2(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded87c15",
   "metadata": {},
   "source": [
    "Note: The output of f2() is sorted list of distinct items. \n",
    "\n",
    "In practice, we often go need to go one step further and do some fine-grained selection of words. In Python we can use list comprehentsion as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d04dda19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a']\n"
     ]
    }
   ],
   "source": [
    "V = set(l)\n",
    "\n",
    "small_voc = [w for w in V if len(w) < 3]\n",
    "\n",
    "print(sorted(small_voc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329ecbd9",
   "metadata": {},
   "source": [
    "## Peformance\n",
    "\n",
    "To get an idea of some candidate solutions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "95741d8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.5 µs ± 456 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)\n"
     ]
    }
   ],
   "source": [
    "import timeit\n",
    "\n",
    "\n",
    "%timeit f1(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3b7e07e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.9 µs ± 49.1 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%timeit f2(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ae79f8",
   "metadata": {},
   "source": [
    "### Machine Learning approach\n",
    "\n",
    "Solution: First tokenize the string using nltk word_tokenizer() method then compute the frequency distribution that will tells us the frequency of each vocabulary item in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fc02479a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d35ebcbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['every',\n",
       " 'person',\n",
       " 'had',\n",
       " 'a',\n",
       " 'star',\n",
       " ',',\n",
       " 'and',\n",
       " 'every',\n",
       " 'star',\n",
       " 'had',\n",
       " 'a',\n",
       " 'also',\n",
       " 'a',\n",
       " 'friend']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = word_tokenize(s)\n",
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4e5fd0ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'a': 3, 'every': 2, 'had': 2, 'star': 2, 'person': 1, ',': 1, 'and': 1, 'also': 1, 'friend': 1})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.FreqDist(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "54948dcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['every', 'person', 'had', 'a', 'star', ',', 'and', 'also', 'friend'])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.FreqDist(l).keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5aca50c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Solution 3:\n",
    "\n",
    "def f3(s):\n",
    "    l = word_tokenize(s)\n",
    "    return nltk.FreqDist(l).keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "598d4d9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['every', 'person', 'had', 'a', 'star', ',', 'and', 'also', 'friend'])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f3(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "34f6e057",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "240 µs ± 5.43 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%timeit f3(s)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0cc1a0",
   "metadata": {},
   "source": [
    "Note: The output of f3() are dict_keys - distinct items. \n",
    "\n",
    "In practice, we often do more fine-grained selection of words, fitering using list comprehentsion as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "28c6297a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['person', ',', 'and', 'also', 'friend']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[w for w in l if nltk.FreqDist(l)[w] < 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad090a50",
   "metadata": {},
   "source": [
    "# REMARKS\n",
    "\n",
    "\n",
    "\n",
    "PS: I suggested the ML solution during a tech interview, but it was not accepted \n",
    "\n",
    "I disagree with this philosophy, as I believe that, in general, if we use \"off-the-shelf\" highly-optimized Machine Learning algorithms can turn any programmer into a better one. Specifically, using powerful NLP methods for processing raw text stored as strings can help us manipulate, analyse and interpret strings more efficiently, even text as large as the size of an entire book (more than 1 million characters).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
